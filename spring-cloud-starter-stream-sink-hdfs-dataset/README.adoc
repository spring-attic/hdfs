//tag::ref-doc[]
= HDFS Dataset Sink

This module writes each message it receives to HDFS as part of a Kite SDK Dataset.

== Input

=== Headers

* `content-type: application/x-java-serialized-object`

=== Payload

* `java.io.Serializable`

== Output

N/A

== Options

The **$$hdfs-dataset$$** $$sink$$ has the following options:

//tag::configuration-properties[]
$$hdfs.dataset.allow-null-values$$:: $$Whether null property values are allowed, if set to true then schema will use UNION for each field.$$ *($$Boolean$$, default: `$$false$$`)*
$$hdfs.dataset.batch-size$$:: $$Threshold in number of messages when file will be automatically flushed and rolled over.$$ *($$Integer$$, default: `$$10000$$`)*
$$hdfs.dataset.compression-type$$:: $$Compression type name (snappy, deflate, bzip2 (avro only) or uncompressed)$$ *($$String$$, default: `$$snappy$$`)*
$$hdfs.dataset.directory$$:: $$The base directory path where the files will be written in the Hadoop FileSystem.$$ *($$String$$, default: `$$/tmp/hdfs-dataset-sink$$`)*
$$hdfs.dataset.format$$:: $$The format to use, valid options are avro and parquet.$$ *($$String$$, default: `$$avro$$`)*
$$hdfs.dataset.fs-uri$$:: $$The URI to use to access the Hadoop FileSystem.$$ *($$String$$, default: `$$<none>$$`)*
$$hdfs.dataset.idle-timeout$$:: $$Idle timeout in milliseconds when Hadoop file resource is automatically closed.$$ *($$Long$$, default: `$$-1$$`)*
$$hdfs.dataset.namespace$$:: $$The sub-directory under the base directory where files will be written.$$ *($$String$$, default: `$$<none>$$`)*
$$hdfs.dataset.partition-path$$:: $$The partition path strategy to use, a list of KiteSDK partition expressions separated by a '/' symbol.$$ *($$String$$, default: `$$<none>$$`)*
$$hdfs.dataset.writer-cache-size$$:: $$The size of the cache to be used for partition writers (10 if omitted).$$ *($$Integer$$, default: `$$-1$$`)*
//end::configuration-properties[]

NOTE: This module can have it's runtime dependencies provided during startup if you would like to use a Hadoop distribution other than the default one.

== Build

```
$ ./mvnw clean install -PgenerateApps
$ cd apps
```
You can find the corresponding binder based projects here.
You can then cd into one of the folders and build it:
```
$ ./mvnw clean package
```

== Examples

```
java -jar hdfs-dataset-sink.jar
```

//end::ref-doc[]
